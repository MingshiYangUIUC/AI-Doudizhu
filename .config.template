; Change this file to ".config.ini" to use.




[TRAIN]
; Used by train_main.py


; Master Arguments about Training

; Auto train mode: continue iterating for task episode number
auto = True

; Task episode number: selfplay and train until this number of games are played
train = 50000000

; Query status mode: return most recent model version and do not train
query = False

; Log training status: save training log to a txt file
; A log includes time, number of games, selfplay statistics, number of timesteps, hyperparameters, and mode losses.
logging = True

; Save training data? [for transfer learning. 1: gather SL_X, SL_Y, QV_Y. 2: train SLM. 3: SL_X+SL_y_hat = QV_x_hat. 4: train QV]
; FYI: Transfer learning does not seem to create strong enough model, so no code is provided to TL
savedata = False


; Model Arguments

; Version of the model, will create new model if does not exist
version = V2_3.0

; Number of historic moves in model input
n_history = 15

; Additional feature size of the model, should not change unless you want to add features
; check "get_action_serial_*" functions in "model_utils.py" and functions in "base_funcs_selfplay.py" for detail
n_feature = 7

; Model parameter 0: SLM LSTM hidden layer size
m_par0 = 256

; Model parameter 1: SLM MLP hidden layer size
m_par1 = 512

; Model parameter 2: QV MLP hidden layer size
m_par2 = 512

; Model init seed: random seed for model initialization, this is my favorite default value lol
m_seed = 20010101


; Environment Arguments

; Device for selfplay games: cpu or cuda gpu
selfplay_device = cuda

; Number of games to be played before each round of saving: save when # of episodes % n_save == 0
n_save = 500000

; Number of games to be played before restarting mp pool. 
; Frequently refresh the workers when # of episodes % n_refresh == 0 to prevent garbage accumulation and other bugs
; Less frequently refresh to reduce worker initialization overhead
n_refresh = 500000

; Number of CPU processes used in selfplay
; These processes will retain a copy of model and connect with GPU, so vram usage will increase.
n_processes = 12

; Batch number of concurrent games send to GPU by each process
; Higher batch size tends to increase efficiency if gpu permits
selfplay_batch_size = 256

; num_workers in data loader
n_worker = 0

; Max true batch size send to model in each process: equals selfplay_batch_size X number of actions
; limit this to increase stability as number of actions cound be insanely large, leading to oom.
; Tune this based on rule of thumb: on average, number of legal actions ~100 in the first turn, and < 30 later.
max_inf_bs = 30


; Hyperparameters and other adjustments

; Batch size for model training
batch_size = 4096

; Number of games to be played before each round of training: play this many games to generate training data
; Increase for stability of training, but use more memory and may lead to unexpected behavior (as of my testing)
n_episodes = 25000

; Number of games to be played before each checkpoint model update. checkpoint model is used in selfplay, updated less often.
; A less costly way to increase effective n_episodes
n_checkpoint = 100000

; In early stage, timesteps per game is large, limit this to prevent too many timesteps generated by selfplay per training round
; Reduce risk of oom, etc. This is not strict limit, might overshoot by the last one game. Correspond to number of rows for training QV model.
timestep_limit = 1100000

; Number of epochs for training. 
; Set to 1 to always use fresh data and prevent overfitting
n_epoch = 1

; Scaled (batch_size=64) Learning rate for model part 1
; true lr = batch_size / 64 * lr1, the true value will be logged if enabled
lr1 = 0.0000001

; Scaled (batch_size=64) Learning rate for model part 2
; true lr = batch_size / 64 * lr2
lr2 = 0.00001

; L2 regularization strength, weight decay parameter in Adam optimizer
l2_reg_strength = 1e-8

; Dropout rate of both models
dropout = 0.1

; Random parameter for action selection (>=0). p(action) = softmax( Q(action) / rand_param )
rand_param = 0.01

; Chance of getting a deck full of bombs during selfplay (0-1)
; change this to high value for fun
bomb_chance = 0.00




[MATCH]
; used by model_match_fast.py


; Model match arguments

; Version of gating model, please check the state dict parameters below
v_gate = H15-V2_3.0

; Model parameter 0: SLM LSTM for gating model
mg_par0 = 256

; Model parameter 1: SLM MLP for gating model
mg_par1 = 512

; Model parameter 2: QV MLP for gating model
mg_par2 = 512

; Iteration number of gating model
i_gate = 

; Version of model series
v_series = H15-V2_3.0

; Model parameter 0: SLM LSTM for model series
ms_par0 = 256

; Model parameter 1: SLM MLP for model series
ms_par1 = 512

; Model parameter 2: QV MLP for model series
ms_par2 = 512

; Start iteration number
i_start = 

; Stop iteration number (inclusive)
i_stop = 

; Iteration number step. 
; In contest, model series identifiers = range(start, stop+1, step)
i_step = 

; Number of games per pair of model (AxB and BxA are two pairs)
n_game = 10000

; Max number of CPU processes used in selfplay, ideally one process per task. Will be lower if there are not enough tasks
; Total task number is 2 * number of model series.
n_processes = 12

; Batch number of concurrent game states send to GPU by each process
selfplay_batch_size = 256

; Device for selfplay games / matches
selfplay_device = cuda




[PVC]
; used by pvc.py


; PVC arguments

; Version of model
version = H15-V2_3.0Bz

; Model parameter 0: SLM LSTM
m_par0 = 256

; Model parameter 1: SLM MLP
m_par1 = 512

; Model parameter 2: QV MLP
m_par2 = 512

; Enable automatic mode: three AI playing and you just spectate one of them.
automatic = True

; Enable bomb mode: players are more likely to get bombs.
bombmode = False

; Show all statistics regardless of game mode. 
; Statistics include predicted cards of other players, candidate actions and associated scores.
showall = True

; Role number. -1: random, 0: Landlord, 1: Farmer-0, 2: Farmer-1. 
role = -1

; Softmax temperature: randomness of AI actions (float >= 0)
temperature = 0.0

; Difficulty level as quality of initial cards. 1: excellent, 2: good, 3: fair, 4: poor, 5: terrible. empty: random
difficulty = 

; seed to initialize the game, does not work with bombmode or difficulty. empty: random
seed = 

; pausetime: manual of delay of displaying new console outputs
pausetime = 0.1
